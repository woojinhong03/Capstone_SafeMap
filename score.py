import pandas as pd
import numpy as np
from math import radians, cos, sin, asin, sqrt
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import warnings
import sys
import os
import json

# ì§€ë„ ì‹œê°í™”ë¥¼ ìœ„í•œ ì¶”ê°€ import
try:
    import osmnx as ox
    import networkx as nx
    from geopy.distance import geodesic
    import folium
    import requests
    import webbrowser
    from dotenv import load_dotenv
    from scipy.spatial import cKDTree
    VISUALIZATION_AVAILABLE = True
except ImportError:
    VISUALIZATION_AVAILABLE = False
    print("ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ìœ„í—˜ë„ ë¶„ì„ë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")

warnings.filterwarnings('ignore')

if sys.platform == 'win32':
    import locale
    try:
        os.system('chcp 65001 > nul')
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

if VISUALIZATION_AVAILABLE:
    load_dotenv()  # env íŒŒì¼ì—ì„œ Kakao API key ë¶ˆëŸ¬ì˜¤ê¸°

class PedestrianRiskPredictor:
    def __init__(self):
        try:
            self.data = pd.read_csv('êµí†µì‚¬ê³ ë‹¤ë°œêµ¬ì—­.csv', encoding='utf-8')
        except UnicodeDecodeError:
            try:
                self.data = pd.read_csv('êµí†µì‚¬ê³ ë‹¤ë°œêµ¬ì—­.csv', encoding='cp949')
            except UnicodeDecodeError:
                self.data = pd.read_csv('êµí†µì‚¬ê³ ë‹¤ë°œêµ¬ì—­.csv', encoding='euc-kr')
        self.model = None
        self.scaler = MinMaxScaler()
        self.prepare_data()


    def haversine_distance(self, lat1, lon1, lat2, lon2):
        try:
            lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])

            dlat = lat2 - lat1
            dlon = lon2 - lon1
            a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
            c = 2 * asin(sqrt(a))
            r = 6371
            return c * r
        except:
            return float('inf')

    def prepare_data(self):
        print("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")

        self.data = self.data.dropna(subset=['X', 'Y'])

        self.data['X'] = pd.to_numeric(self.data['X'], errors='coerce')
        self.data['Y'] = pd.to_numeric(self.data['Y'], errors='coerce')
        self.data['OCCU_TM'] = pd.to_numeric(self.data['OCCU_TM'], errors='coerce')

        self.data = self.data.dropna(subset=['X', 'Y', 'OCCU_TM'])

        severity_scores = {
            'ê²½ìƒ': 1,
            'ë¶€ìƒì‹ ê³ ': 2,
            'ì¤‘ìƒ': 4,
            'ì‚¬ë§': 10
        }
        self.data['severity_score'] = self.data['LCLAS'].map(severity_scores).fillna(1)

        self.data['WLKG'] = self.data['WLKG'].astype(str)
        self.data['SCLAS'] = self.data['SCLAS'].astype(str)

        pedestrian_condition = (
            (self.data['SCLAS'].str.contains('ì°¨ëŒ€ì‚¬ëŒ', na=False)) |
            (self.data['WLKG'].str.contains('O', na=False))
        )
        self.data['pedestrian_weight'] = np.where(pedestrian_condition, 3, 1)

        def get_time_period(hour):
            try:
                hour = int(hour)
                if 0 <= hour < 6:
                    return 'ìƒˆë²½'
                elif 6 <= hour < 12:
                    return 'ì˜¤ì „'
                elif 12 <= hour < 18:
                    return 'ì˜¤í›„'
                else:
                    return 'ì €ë…'
            except:
                return 'ì˜¤ì „'

        self.data['time_period'] = self.data['OCCU_TM'].apply(get_time_period)

        weekend_days = ['í† ìš”ì¼', 'ì¼ìš”ì¼']
        self.data['is_weekend'] = self.data['OCCU_DAY'].isin(weekend_days).astype(int)

        print(f"ì „ì²´ ì‚¬ê³  ë°ì´í„°: {len(self.data)}ê±´")
        print(f"ë³´í–‰ì ê´€ë ¨ ì‚¬ê³ : {len(self.data[self.data['pedestrian_weight'] == 3])}ê±´")

        print("\në°ì´í„° ìƒ˜í”Œ:")
        print(self.data[['LCLAS', 'SCLAS', 'WLKG', 'severity_score', 'pedestrian_weight']].head())

    def calculate_risk_features(self, target_lat, target_lon, radius_km=0.5):
        
        try:
            distances = self.data.apply(
                lambda row: self.haversine_distance(target_lat, target_lon, row['Y'], row['X']),
                axis=1
            )
            nearby_accidents = self.data[distances <= radius_km].copy()

            if len(nearby_accidents) == 0:
                return {
                    'total_accidents': 0,
                    'pedestrian_accidents': 0,
                    'severity_weighted_score': 0,
                    'pedestrian_weighted_score': 0,
                    'weekend_accidents': 0,
                    'night_accidents': 0,
                    'fatal_accidents': 0
                }

            night_condition = (
                (nearby_accidents['OCCU_TM'] >= 22) |
                (nearby_accidents['OCCU_TM'] <= 6)
            )
            night_accidents = len(nearby_accidents[night_condition])

            fatal_condition = nearby_accidents['LCLAS'].str.contains('ì‚¬ë§', na=False)
            fatal_accidents = len(nearby_accidents[fatal_condition])

            features = {
                'total_accidents': len(nearby_accidents),
                'pedestrian_accidents': len(nearby_accidents[nearby_accidents['pedestrian_weight'] == 3]),
                'severity_weighted_score': (nearby_accidents['severity_score'] * nearby_accidents['pedestrian_weight']).sum(),
                'pedestrian_weighted_score': nearby_accidents['pedestrian_weight'].sum(),
                'weekend_accidents': nearby_accidents['is_weekend'].sum(),
                'night_accidents': night_accidents,
                'fatal_accidents': fatal_accidents
            }

            return features

        except Exception as e:
            print(f"íŠ¹ì„± ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'total_accidents': 0,
                'pedestrian_accidents': 0,
                'severity_weighted_score': 0,
                'pedestrian_weighted_score': 0,
                'weekend_accidents': 0,
                'night_accidents': 0,
                'fatal_accidents': 0
            }

    def create_training_data(self, n_samples=800):
        
        print("í›ˆë ¨ ë°ì´í„° ìƒì„± ì¤‘...")

        sample_size = min(n_samples // 2, len(self.data))
        sample_indices = np.random.choice(len(self.data), sample_size, replace=False)
        sample_locations = self.data.iloc[sample_indices][['Y', 'X']].values

        lat_min, lat_max = self.data['Y'].min(), self.data['Y'].max()
        lon_min, lon_max = self.data['X'].min(), self.data['X'].max()

        additional_samples = n_samples - len(sample_locations)
        if additional_samples > 0:
            random_lats = np.random.uniform(lat_min, lat_max, additional_samples)
            random_lons = np.random.uniform(lon_min, lon_max, additional_samples)
            random_locations = np.column_stack([random_lats, random_lons])
            sample_locations = np.vstack([sample_locations, random_locations])

        training_features = []
        training_targets = []

        for i, (lat, lon) in enumerate(sample_locations):
            if i % 100 == 0:
                print(f"ì§„í–‰ë¥ : {i}/{len(sample_locations)}")

            features = self.calculate_risk_features(lat, lon)

            import math

            # ë” ì„¸ë°€í•œ ê°€ì¤‘ì¹˜ ì ìš©ìœ¼ë¡œ ì ìˆ˜ ë¶„í¬ë¥¼ ê°œì„ 
            total_weight = (
                features['total_accidents'] * 0.05 +
                features['pedestrian_accidents'] * 0.3 +
                features['fatal_accidents'] * 0.8 +
                features['night_accidents'] * 0.1 +
                features['weekend_accidents'] * 0.05
            )

            # ì‹¬ê°ë„ ê°€ì¤‘ì¹˜ë¥¼ ë‚®ê²Œ ì¡°ì •
            severity_factor = features['severity_weighted_score'] * 0.02

            # ìµœì¢… ì ìˆ˜ ê³„ì‚° (1-10 ë²”ìœ„ë¡œ ë” ë¶€ë“œëŸ½ê²Œ ë¶„ë°°)
            raw_score = 1.0 + total_weight + severity_factor

            # ì ìˆ˜ë¥¼ 1-10 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§ (ë¡œê·¸ í•¨ìˆ˜ë¡œ ë¶€ë“œëŸ½ê²Œ)
            if raw_score > 1:
                risk_score = 1.0 + math.log(raw_score) * 1.8
            else:
                risk_score = 1.0

            risk_score = max(1.0, min(10.0, risk_score))

            training_features.append(list(features.values()))
            training_targets.append(risk_score)

        return np.array(training_features), np.array(training_targets)

    def train_model(self, n_samples=800):
        
        print("ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        try:
            X, y = self.create_training_data(n_samples)

            X_scaled = self.scaler.fit_transform(X)

            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42
            )

            self.model = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
            self.model.fit(X_train, y_train)

            train_score = self.model.score(X_train, y_train)
            test_score = self.model.score(X_test, y_test)

            print(f"í›ˆë ¨ ì ìˆ˜: {train_score:.4f}")
            print(f"í…ŒìŠ¤íŠ¸ ì ìˆ˜: {test_score:.4f}")
            print("ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")

        except Exception as e:
            print(f"ëª¨ë¸ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def predict_risk(self, latitude, longitude):
        
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

        try:
            features = self.calculate_risk_features(latitude, longitude)

            X_pred = np.array([list(features.values())])
            X_pred_scaled = self.scaler.transform(X_pred)
            risk_score = self.model.predict(X_pred_scaled)[0]

            # ì ìˆ˜ë¥¼ ì ì ˆí•œ ë²”ìœ„ë¡œ ë¶„ë°°
            risk_score = max(1, min(10, risk_score))

            return {
                'risk_score': round(risk_score, 1),
                'risk_level': self.get_risk_level(risk_score),
                'nearby_accidents': features['total_accidents'],
                'pedestrian_accidents': features['pedestrian_accidents'],
                'details': features
            }

        except Exception as e:
            print(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                'risk_score': 1.0,
                'risk_level': "ë§¤ìš° ë‚®ìŒ",
                'nearby_accidents': 0,
                'pedestrian_accidents': 0,
                'details': {}
            }

    def get_risk_level(self, score):
        
        if score <= 2:
            return "ë§¤ìš° ë‚®ìŒ"
        elif score <= 4:
            return "ë‚®ìŒ"
        elif score <= 6:
            return "ë³´í†µ"
        elif score <= 8:
            return "ë†’ìŒ"
        else:
            return "ë§¤ìš° ë†’ìŒ"

    def analyze_rectangular_area(self, lat_a, lon_a, lat_b, lon_b, grid_distance_m=75):
        
        import json

        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

        center_lat = (lat_a + lat_b) / 2
        center_lon = (lon_a + lon_b) / 2

        half_lat = abs(lat_a - lat_b) / 2
        half_lon = abs(lon_a - lon_b) / 2

        min_lat = center_lat - half_lat * 3
        max_lat = center_lat + half_lat * 3
        min_lon = center_lon - half_lon * 3
        max_lon = center_lon + half_lon * 3

        print(f"ë¶„ì„ ì§€ì—­: ({min_lat:.6f}, {min_lon:.6f}) ~ ({max_lat:.6f}, {max_lon:.6f})")

        lat_step = grid_distance_m / 111000
        avg_lat = (min_lat + max_lat) / 2
        lon_step = grid_distance_m / (111000 * np.cos(np.radians(avg_lat)))

        print(f"ê²©ì ê°„ê²©: ìœ„ë„ {lat_step:.6f}ë„, ê²½ë„ {lon_step:.6f}ë„")

        lat_points = int((max_lat - min_lat) / lat_step) + 1
        lon_points = int((max_lon - min_lon) / lon_step) + 1
        total_points = lat_points * lon_points

        print(f"ì´ {total_points}ê°œ ê²©ìì  ë¶„ì„ ì˜ˆì •")

        danger_centers = []
        grid_count = 0

        lat = min_lat
        while lat <= max_lat:
            lon = min_lon
            while lon <= max_lon:
                result = self.predict_risk(lat, lon)

                weight = round(result['risk_score'], 1)

                danger_centers.append({
                    "lat": round(lat, 6),
                    "lon": round(lon, 6),
                    "weight": weight
                })

                grid_count += 1
                if grid_count % 50 == 0:
                    print(f"ì§„í–‰ë¥ : {grid_count}/{total_points} ê²©ìì  ì²˜ë¦¬ ì™„ë£Œ ({grid_count/total_points*100:.1f}%)")

                lon += lon_step
            lat += lat_step

        print(f"ì´ {len(danger_centers)}ê°œ ê²©ìì  ë¶„ì„ ì™„ë£Œ")

        danger_centers.sort(key=lambda x: (x['lat'], x['lon']))

        return danger_centers

    def save_danger_centers_json(self, danger_centers, filename="danger_centers_output.json"):
        
        import json

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(danger_centers, f, ensure_ascii=False, indent=4)

        print(f"ê²°ê³¼ê°€ {filename} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    try:
        print("ë³´í–‰ì ìœ„í—˜ë„ ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        predictor = PedestrianRiskPredictor()
        predictor.train_model(n_samples=600)

        test_locations = [
            (37.570, 126.982),  # ì¢…ê°ì—­
            (37.5814, 126.9880),  # ëŒ€ë™ì„¸ë¬´ê³ ë“±í•™êµ
            (37.5665, 126.9780),  # ì„œìš¸ì—­
        ]

        print("\n=== ë³´í–‰ì ìœ„í—˜ë„ ì˜ˆì¸¡ ê²°ê³¼ ===")
        for i, (lat, lon) in enumerate(test_locations, 1):
            result = predictor.predict_risk(lat, lon)
            print(f"\nìœ„ì¹˜ {i}: ìœ„ë„ {lat}, ê²½ë„ {lon}")
            print(f"ìœ„í—˜ë„ ì ìˆ˜: {result['risk_score']}/10")
            print(f"ìœ„í—˜ ìˆ˜ì¤€: {result['risk_level']}")
            print(f"ì£¼ë³€ ì´ ì‚¬ê³  ê±´ìˆ˜: {result['nearby_accidents']}ê±´")
            print(f"ë³´í–‰ì ê´€ë ¨ ì‚¬ê³ : {result['pedestrian_accidents']}ê±´")

        print("\n=== ì§ì ‘ ì¢Œí‘œ ì…ë ¥ ===")
        print("ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")

        example_coords = [
            (37.570, 126.982, "ì¢…ê°ì—­"),
            (37.5814, 126.9880, "ëŒ€ë™ì„¸ë¬´ê³ ë“±í•™êµ"),
            (37.5665, 126.9780, "ì„œìš¸ì—­"),
            (37.4979, 127.0276, "ê°•ë‚¨ì—­")
        ]

        print("\n=== ì¶”ê°€ í…ŒìŠ¤íŠ¸ ìœ„ì¹˜ ===")
        for lat, lon, name in example_coords:
            result = predictor.predict_risk(lat, lon)
            print(f"\nğŸ“ {name} ({lat}, {lon}):")
            print(f"ğŸ”¢ ìœ„í—˜ë„ ì ìˆ˜: {result['risk_score']}/10")
            print(f"âš ï¸  ìœ„í—˜ ìˆ˜ì¤€: {result['risk_level']}")
            print(f"ğŸ“Š ì£¼ë³€ ì‚¬ê³  ë¶„ì„:")
            print(f"   â€¢ ì´ ì‚¬ê³  ê±´ìˆ˜: {result['nearby_accidents']}ê±´")
            print(f"   â€¢ ë³´í–‰ì ê´€ë ¨ ì‚¬ê³ : {result['pedestrian_accidents']}ê±´")
            print(f"   â€¢ ì¹˜ëª…ì  ì‚¬ê³ : {result['details'].get('fatal_accidents', 0)}ê±´")
            print(f"   â€¢ ì•¼ê°„ ì‚¬ê³ : {result['details'].get('night_accidents', 0)}ê±´")
            print(f"   â€¢ ì£¼ë§ ì‚¬ê³ : {result['details'].get('weekend_accidents', 0)}ê±´")

        print("\nì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤!")

        print("\n=== ì‚¬ê°í˜• ì§€ì—­ ìœ„í—˜ë„ ë¶„ì„ ===")
        print("A, B ë‘ ì§€ì ì˜ ì¢Œí‘œë¥¼ ì…ë ¥í•˜ì—¬ ì‚¬ê°í˜• ì§€ì—­ì˜ ìœ„í—˜ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

        try:
            print("\nğŸ“ ì¢Œí‘œ ì…ë ¥ (í˜•ì‹: ìœ„ë„,ê²½ë„):")
            a_input = input("A ì§€ì  ì¢Œí‘œ: ")
            lat_a, lon_a = map(float, a_input.split(','))

            b_input = input("B ì§€ì  ì¢Œí‘œ: ")
            lat_b, lon_b = map(float, b_input.split(','))

            print(f"\në¶„ì„ ì„¤ì •:")
            print(f"A ì§€ì : ({lat_a}, {lon_a})")
            print(f"B ì§€ì : ({lat_b}, {lon_b})")
            print("75m ê°„ê²©ìœ¼ë¡œ ìœ„í—˜ë„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

            danger_centers = predictor.analyze_rectangular_area(lat_a, lon_a, lat_b, lon_b)

            predictor.save_danger_centers_json(danger_centers)

            print(f"\nğŸ“Š ì²« 5ê°œ ê²©ìì  ì •ë³´:")
            for i, center in enumerate(danger_centers[:5], 1):
                print(f"{i}. ìœ„ë„: {center['lat']}, ê²½ë„: {center['lon']}, ìœ„í—˜ë„: {center['weight']}/10")

            print(f"\nâœ… ì´ {len(danger_centers)}ê°œ ê²©ìì  ë¶„ì„ ì™„ë£Œ")
            print("ğŸ“„ ê²°ê³¼ê°€ danger_centers_output.json íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        except ValueError:
            print("âŒ ì˜¬ë°”ë¥¸ ìˆ«ì í˜•ì‹ì˜ ì¢Œí‘œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    except Exception as e:
        print(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("CSV íŒŒì¼ ê²½ë¡œì™€ ë°ì´í„° í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
